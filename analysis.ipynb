{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Word2Vec, KeyedVectors, LdaModel\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/DisneylandReviews.csv'\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_info = pd.DataFrame({'Missing Values': missing_values, 'Missing Percentage': missing_percentage})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Rating Distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=df, x='Rating', hue='Rating', dodge=False, legend=False)\n",
    "plt.title('Rating Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Rating')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/rating_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Reviews per Disneyland Branch\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, y='Branch', hue='Branch', dodge=False, order=df['Branch'].value_counts().index,\n",
    "                legend=False)\n",
    "plt.title('Reviews per Disneyland Branch')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Branch')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/branch_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Review Length Distribution\n",
    "df['Review_Length'] = df['Review_Text'].apply(len)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df['Review_Length'], bins=50, kde=True, color='blue')\n",
    "plt.title('Review Length Distribution')\n",
    "plt.xlabel('Length of Review')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/review_length.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Reviews per Year\n",
    "df['Year'] = df['Year_Month'].apply(lambda x: x.split('-')[0] if x != 'missing' else 'Unknown')\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='Year', hue='Year', dodge=False, order=sorted(df['Year'].unique()), legend=False)\n",
    "plt.title('Reviews per Year')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Year')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/year_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 5: Reviews per Reviewer Location (Top 10)\n",
    "top_locations = df['Reviewer_Location'].value_counts().head(10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_locations, y=top_locations.index, hue=top_locations.index, dodge=False, legend=False)\n",
    "plt.title('Top 10 Reviewer Locations')\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.ylabel('Reviewer Location')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/top_locations.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 6: Rating Over Years by Branch\n",
    "rating_per_year_branch = df.groupby(['Year', 'Branch'])['Rating'].mean().unstack()\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(rating_per_year_branch, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Rating Over Years by Branch')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/rating_over_years_by_branch.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 7: Wordcloud for positive reviews with 4 or 5 rating\n",
    "positive_reviews = df[df['Rating'] >= 4]\n",
    "positive_reviews_text = ' '.join(positive_reviews['Review_Text'])\n",
    "wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110, background_color='white',\n",
    "                        max_words=100).generate(positive_reviews_text)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.title('Wordcloud for Positive Reviews')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/wordcloud_positive_reviews.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 8: Wordcloud for negative reviews with 1, 2 or 3 rating\n",
    "negative_reviews = df[df['Rating'] <= 3]\n",
    "negative_reviews_text = ' '.join(negative_reviews['Review_Text'])\n",
    "wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110, background_color='white',\n",
    "                        max_words=100).generate(negative_reviews_text)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.title('Wordcloud for Negative Reviews')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/wordcloud_negative_reviews.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
    "# Disabling unnecessary components for efficiency\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stopwords\n",
    "custom_stopwords = {\n",
    "    'disney', 'land', 'disneyland', 'rides', 'ride', 'good', 'really', 'very', 'quite',\n",
    "    'pretty', 'especially', 'actually', 'probably', 'maybe', 'sure', 'time', 'day', 'year',\n",
    "    'thing', 'world', 'point', 'bit', 'number', 'week', 'make', 'say', 'come', 'go', 'know',\n",
    "    'take', 'see', 'get', 'want', 'think', 'look', 'tell', 'try', 'use', 'need', 'feel',\n",
    "    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my',\n",
    "    'your', 'his', 'its', 'our', 'their', 'a', 'an', 'the', 'in', 'on', 'at', 'from', 'with',\n",
    "    'about', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below',\n",
    "    'over', 'under', 'to', 'of', 'for', 'by', 'and', 'but', 'or', 'so', 'yet', 'because',\n",
    "    'as', 'until', 'than', '10', '20', '30', '45', '15', 'minute', 'second', 'hour', 'day', 'pm'\n",
    "                                                                                            'park', 'go', 'one', 'kid'\n",
    "}\n",
    "\n",
    "# Update stop words list\n",
    "stop_words = set(stopwords.words('english')).union(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rating_threshold = 3\n",
    "clean_df = df[df['Rating'] <= low_rating_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling negations by creating bi-grams with negation word and subsequent word.\n",
    "def handle_negations(text):\n",
    "    # Define the negation pattern\n",
    "    negation_pattern = re.compile(\n",
    "        r\"\\b(not|no|never|none|cannot|can't|couldn't|shouldn't|won't|wouldn't|don't|doesn't|didn't|isn't|aren't|ain't\"\n",
    "        r\")\\s([a-z]+)\\b\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    negated_form = r'\\1_\\2'  # E.g., \"not_good\"\n",
    "    return negation_pattern.sub(negated_form, text)\n",
    "\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Normalize text to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove punctuation\n",
    "    text = handle_negations(text)  # Handle negations\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stop words\n",
    "    lemmatized = nlp(' '.join(tokens))  # Lemmatization\n",
    "    lemmatized = [token.lemma_ for token in lemmatized]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "\n",
    "# Apply preprocessing to the Review_Text column of the DataFrame\n",
    "clean_df['Clean_Text'] = clean_df['Review_Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the processed data\n",
    "print(clean_df[['Review_Text', 'Clean_Text']].head())\n",
    "\n",
    "# Export to a new CSV file\n",
    "clean_df.to_csv('data/cleaned_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW) model + LDA, LSA, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer and fit and transform\n",
    "count_vectorizer = CountVectorizer(stop_words='english', ngram_range=(2, 3), min_df=5, max_df=0.5)\n",
    "count_vectors = count_vectorizer.fit_transform(clean_df['Clean_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "sum_words = count_vectors.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in count_vectorizer.vocabulary_.items()]\n",
    "sorted_words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top N most frequent words\n",
    "top_n = 30\n",
    "print(\"\\nTop {} most frequent words/ngrams:\".format(top_n))\n",
    "print(\"-\" * 40)\n",
    "for word, freq in sorted_words_freq[:top_n]:\n",
    "    print(\"{:<20} : {}\".format(word, freq))\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top N words/ngrams\n",
    "top_n = 30\n",
    "words, freqs = zip(*sorted_words_freq[:top_n])\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(words)), freqs, align='center')\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest frequency on top\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top {} Words/N-grams Frequency'.format(top_n))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of topics and top words to display\n",
    "n_topics = 4\n",
    "no_top_words = 10\n",
    "\n",
    "# Initialize and fit LDA, LSA, and NMF models\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42).fit(count_vectors)\n",
    "lsa = TruncatedSVD(n_components=n_topics).fit(count_vectors)\n",
    "nmf = NMF(n_components=n_topics, random_state=42).fit(count_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic {}:\".format(topic_idx + 1))\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "# Display topics for each model\n",
    "print(\"\\nLDA Model Topics:\")\n",
    "display_topics(lda, count_vectorizer.get_feature_names_out(), no_top_words)\n",
    "print(\"\\nLSA Model Topics:\")\n",
    "display_topics(lsa, count_vectorizer.get_feature_names_out(), no_top_words)\n",
    "print(\"\\nNMF Model Topics:\")\n",
    "display_topics(nmf, count_vectorizer.get_feature_names_out(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a word cloud\n",
    "def generate_word_cloud(topic, feature_names, no_top_words):\n",
    "    word_freqs = {feature_names[i]: topic[i] for i in topic.argsort()[:-no_top_words - 1:-1]}\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freqs)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate word clouds for LDA topics\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Word Cloud for LDA Topic {}:\".format(topic_idx + 1))\n",
    "    generate_word_cloud(topic, count_vectorizer.get_feature_names_out(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE Visualization for LDA\n",
    "def tsne_visualization(model, data):\n",
    "    print(\"\\nPerforming t-SNE Visualization...\")\n",
    "    topic_weights = model.transform(data)\n",
    "    tsne_model = TSNE(n_components=2, verbose=0, random_state=0, angle=.99, init='pca')\n",
    "    tsne_lda = tsne_model.fit_transform(topic_weights)\n",
    "\n",
    "    # Plot the t-SNE visualization\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(tsne_lda[:, 0], tsne_lda[:, 1], alpha=0.5)\n",
    "    plt.title('t-SNE Visualization of LDA Topics')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.show()\n",
    "\n",
    "tsne_visualization(lda, count_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF model + LDA, LSA, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer and fit and transform\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), min_df=5, max_df=0.5)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(clean_df['Clean_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up the TF-IDF scores of each vocabulary word\n",
    "sum_tfidf = tfidf_vectors.sum(axis=0)\n",
    "words_tfidf = [(word, sum_tfidf[0, idx]) for word, idx in tfidf_vectorizer.vocabulary_.items()]\n",
    "sorted_words_tfidf = sorted(words_tfidf, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the top N words with the highest TF-IDF score\n",
    "top_n = 30\n",
    "print(\"\\nTop {} words with the highest TF-IDF scores:\".format(top_n))\n",
    "print(\"-\" * 40)\n",
    "for word, score in sorted_words_tfidf[:top_n]:\n",
    "    print(\"{:<20} : {}\".format(word, score))\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top N words with the highest TF-IDF scores\n",
    "tfidf_words, tfidf_scores = zip(*sorted_words_tfidf[:top_n])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(tfidf_words)), tfidf_scores, align='center')\n",
    "plt.yticks(range(len(tfidf_words)), tfidf_words)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest frequency on top\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top {} Words with Highest TF-IDF Scores'.format(top_n))\n",
    "plt.savefig('images/word_frequencies.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of topics and top words to display\n",
    "n_topics = 4\n",
    "no_top_words = 10\n",
    "\n",
    "# Initialize and fit LDA, LSA, and NMF models\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42).fit(tfidf_vectors)\n",
    "lsa = TruncatedSVD(n_components=n_topics).fit(tfidf_vectors)\n",
    "nmf = NMF(n_components=n_topics, random_state=42).fit(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic {}:\".format(topic_idx + 1))\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "# Display topics for each model\n",
    "print(\"\\nLDA Model Topics:\")\n",
    "display_topics(lda, tfidf_vectorizer.get_feature_names_out(), no_top_words)\n",
    "print(\"\\nLSA Model Topics:\")\n",
    "display_topics(lsa, tfidf_vectorizer.get_feature_names_out(), no_top_words)\n",
    "print(\"\\nNMF Model Topics:\")\n",
    "display_topics(nmf, tfidf_vectorizer.get_feature_names_out(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a word cloud\n",
    "def generate_word_cloud(topic, feature_names, no_top_words):\n",
    "    word_freqs = {feature_names[i]: topic[i] for i in topic.argsort()[:-no_top_words - 1:-1]}\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freqs)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('images/tfidf_word_cloud.png')\n",
    "    plt.show()\n",
    "\n",
    "# Generate word clouds for LDA topics\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Word Cloud for LDA Topic {}:\".format(topic_idx + 1))\n",
    "    generate_word_cloud(topic, tfidf_vectorizer.get_feature_names_out(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE Visualization for LDA\n",
    "def tsne_visualization(model, data):\n",
    "    print(\"\\nPerforming t-SNE Visualization...\")\n",
    "    topic_weights = model.transform(data)\n",
    "    tsne_model = TSNE(n_components=2, verbose=0, random_state=0, angle=.99, init='pca')\n",
    "    tsne_lda = tsne_model.fit_transform(topic_weights)\n",
    "    return tsne_lda\n",
    "\n",
    "# Plot t-SNE\n",
    "def plot_tsne(tsne_results, title):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(tsne_results[:, 0], tsne_results[:, 1], alpha=0.7)\n",
    "    plt.xlabel('t-SNE feature 1')\n",
    "    plt.ylabel('t-SNE feature 2')\n",
    "    plt.title(title)\n",
    "    plt.savefig('images/tfidf_tsne_lda.png')\n",
    "    plt.show()\n",
    "\n",
    "tsne_lda = tsne_visualization(lda, tfidf_vectors)\n",
    "plot_tsne(tsne_lda, 't-SNE Visualization of LDA Topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec model + LDA, LSA, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed reviews\n",
    "preprocessed_reviews = []\n",
    "for review in clean_df['Clean_Text']:\n",
    "    preprocessed_reviews.append(word_tokenize(review))\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(preprocessed_reviews, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Represent reviews as vectors\n",
    "review_vectors = [model.wv[token] for review in preprocessed_reviews for token in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA for topic modeling\n",
    "dictionary = Dictionary(preprocessed_reviews)\n",
    "corpus = [dictionary.doc2bow(review) for review in preprocessed_reviews]\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary)\n",
    "\n",
    "# Save the Word2Vec model\n",
    "model.wv.save('models/word2vec.model')\n",
    "\n",
    "# Print the topics\n",
    "for topic in lda_model.print_topics():\n",
    "    print(f\"Topic {topic[0]}: {topic[1]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "word2vec_model = KeyedVectors.load('word2vec.model')\n",
    "\n",
    "# Use the Word2Vec model to find similar words\n",
    "similar_words = word2vec_model.similar_by_word('line')\n",
    "print(f\"The words similar to 'line' are: {similar_words}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Use the Word2Vec model to find the similarity between two words\n",
    "similarity = word2vec_model.similarity('queue', 'long')\n",
    "print(f\"The similarity between 'queue' and 'long' is {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Word2Vec model using t-SNE\n",
    "# Get the word vectors\n",
    "word_vectors = word2vec_model.vectors\n",
    "\n",
    "# Reduce the dimensionality of the word vectors using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "word_vectors_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Plot the word vectors in 2D\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], marker='o')\n",
    "plt.title('t-SNE visualization of Word2Vec model')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/word2vec_tsne.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using TextBlob and VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sentiment and put it in a new column\n",
    "clean_df['sentiment'] = clean_df['Clean_Text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# convert sentiment to positive or negative and put it in a new column\n",
    "clean_df['sentiment_cat'] = clean_df['sentiment'].apply(lambda x: 'positive' if x > 0 else 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['sentiment'], kde=True, color='skyblue')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/sentiment_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sentiment distribution by category\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='sentiment_cat', hue='sentiment_cat', dodge=False, palette='pastel')\n",
    "plt.title('Sentiment Distribution by Category')\n",
    "plt.xlabel('Sentiment Category')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/sentiment_category_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis using Vader\n",
    "# Create a SentimentIntensityAnalyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to get the sentiment score\n",
    "def get_sentiment_score(text):\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "# Calculate the sentiment score and put it in a new column\n",
    "clean_df['sentiment_vader'] = clean_df['Clean_Text'].apply(get_sentiment_score)\n",
    "\n",
    "# Convert sentiment to positive or negative and put it in a new column\n",
    "clean_df['sentiment_cat_vader'] = clean_df['sentiment_vader'].apply(lambda x: 'positive' if x > 0 else 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sentiment scores from TextBlob and vaderSentiment\n",
    "print(\"The average sentiment score from TextBlob is:\")\n",
    "print(df['sentiment'].mean())\n",
    "print(\"The average sentiment score from vaderSentiment is:\")\n",
    "print(df['sentiment_vader'].mean())\n",
    "\n",
    "# Compare the sentiment categories from TextBlob and vaderSentiment\n",
    "print(\"The sentiment category distribution from TextBlob is:\")\n",
    "print(df['sentiment_cat'].value_counts())\n",
    "print(\"The sentiment category distribution from vaderSentiment is:\")\n",
    "print(df['sentiment_cat_vader'].value_counts())\n",
    "\n",
    "# Compare the standard deviation of the sentiment scores from TextBlob and vaderSentiment\n",
    "print(\"The standard deviation of the sentiment scores from TextBlob is:\")\n",
    "print(df['sentiment'].std())\n",
    "print(\"The standard deviation of the sentiment scores from vaderSentiment is:\")\n",
    "print(df['sentiment_vader'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['sentiment_vader'], kde=True, color='skyblue')\n",
    "plt.title('Sentiment Distribution (vaderSentiment)')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/sentiment_distribution_vader.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sentiment distribution by category\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='sentiment_cat_vader', hue='sentiment_cat_vader', dodge=False, palette='pastel')\n",
    "plt.title('Sentiment Distribution by Category (vaderSentiment)')\n",
    "plt.xlabel('Sentiment Category')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/sentiment_category_distribution_vader.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentiment analysis results\n",
    "clean_df.to_csv('data/sentiment_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the preprocessed reviews\n",
    "df = pd.read_csv('data/DisneylandReviews.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# calculate sentiment and put it in a new column\n",
    "df['sentiment'] = df['Review_Text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# convert sentiment to positive or negative and put it in a new column\n",
    "df['sentiment_cat'] = df['sentiment'].apply(lambda x: 'positive' if x > 0 else 'negative')\n",
    "\n",
    "# Save the sentiment analysis results\n",
    "df.to_csv('data/sentiment_analysis.csv', index=False)\n",
    "\n",
    "# plot the sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['sentiment'], kde=True, color='skyblue')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/sentiment_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# plot the sentiment distribution by category\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='sentiment_cat', hue='sentiment_cat', dodge=False, palette='pastel')\n",
    "plt.title('Sentiment Distribution by Category')\n",
    "plt.xlabel('Sentiment Category')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/sentiment_category_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# Sentiment Analysis using Vader\n",
    "# Create a SentimentIntensityAnalyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to get the sentiment score\n",
    "def get_sentiment_score(text):\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "# Calculate the sentiment score and put it in a new column\n",
    "df['sentiment_vader'] = df['Review_Text'].apply(get_sentiment_score)\n",
    "\n",
    "# Convert sentiment to positive or negative and put it in a new column\n",
    "df['sentiment_cat_vader'] = df['sentiment_vader'].apply(lambda x: 'positive' if x > 0 else 'negative')\n",
    "\n",
    "# Compare the sentiment scores from TextBlob and vaderSentiment\n",
    "print(\"The average sentiment score from TextBlob is:\")\n",
    "print(df['sentiment'].mean())\n",
    "print(\"The average sentiment score from vaderSentiment is:\")\n",
    "print(df['sentiment_vader'].mean())\n",
    "\n",
    "# Compare the sentiment categories from TextBlob and vaderSentiment\n",
    "print(\"The sentiment category distribution from TextBlob is:\")\n",
    "print(df['sentiment_cat'].value_counts())\n",
    "print(\"The sentiment category distribution from vaderSentiment is:\")\n",
    "print(df['sentiment_cat_vader'].value_counts())\n",
    "\n",
    "# Compare the standard deviation of the sentiment scores from TextBlob and vaderSentiment\n",
    "print(\"The standard deviation of the sentiment scores from TextBlob is:\")\n",
    "print(df['sentiment'].std())\n",
    "print(\"The standard deviation of the sentiment scores from vaderSentiment is:\")\n",
    "print(df['sentiment_vader'].std())\n",
    "\n",
    "# plot the sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['sentiment_vader'], kde=True, color='skyblue')\n",
    "plt.title('Sentiment Distribution (vaderSentiment)')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/sentiment_distribution_vader.png')\n",
    "plt.show()\n",
    "\n",
    "# plot the sentiment distribution by category\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='sentiment_cat_vader', hue='sentiment_cat_vader', dodge=False, palette='pastel')\n",
    "plt.title('Sentiment Distribution by Category (vaderSentiment)')\n",
    "plt.xlabel('Sentiment Category')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/sentiment_category_distribution_vader.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save the sentiment analysis results\n",
    "df.to_csv('data/sentiment_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
